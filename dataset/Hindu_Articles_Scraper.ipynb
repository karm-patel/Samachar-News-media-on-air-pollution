{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import urllib3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize\n",
    "import multiprocessing as mp\n",
    "import traceback\n",
    "import pickle\n",
    "import psutil\n",
    "import time\n",
    "from newspaper import Article\n",
    "import os\n",
    "from newspaper import Config\n",
    "config = Config()\n",
    "config.request_timeout = 60\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "ArticleData = \"ArticlesData/Hindu/\"\n",
    "Path(ArticleData).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Airquality = ArticleData+\"Airquality/\"\n",
    "Path(Airquality).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ContentNotFound = ArticleData+\"ContentNotFound/\"\n",
    "Path(ContentNotFound).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Exceptions = ArticleData+\"Exceptions/\"\n",
    "Path(Exceptions).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HtmlResponses = ArticleData + \"HtmlResponses/\"\n",
    "Path(HtmlResponses).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "HomeExceptions = ArticleData + \"HomeExceptions/\"\n",
    "Path(HomeExceptions).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "LOGGING_ENABLED = False\n",
    "if LOGGING_ENABLED:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    file_handler = logging.FileHandler('hindu_2.log', 'a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    print = logger.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindu Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html response of the page where url are listed of specific date\n",
    "def hindu_get_article_list_html_resp(datetime_obj):\n",
    "    \n",
    "    #make a dynamic link\n",
    "    year = datetime_obj.year\n",
    "    month = datetime_obj.month\n",
    "    day = datetime_obj.day\n",
    "    link = f\"https://www.thehindu.com/archive/web/{year}/{month}/{day}/\"\n",
    "    \n",
    "    #get html response\n",
    "    try:\n",
    "        resp = requests.get(link)\n",
    "        resp_txt = resp.text\n",
    "        return [resp_txt,datetime_obj]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Exception in Home Page @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        return [\"exception\",str(type(e)),traceback.format_exc()]\n",
    "    \n",
    "    \n",
    "#scrape urls of articles,\n",
    "def hindu_get_article_urls(html_resp,datetime_obj):\n",
    "    resp_txt = html_resp\n",
    "    scrapper = bs4.BeautifulSoup(resp_txt,\"html.parser\")\n",
    "    \n",
    "    #matcher of sections which conatains urls of articles\n",
    "    section_matcher = re.compile(\"section_[0-9]+\",flags = re.I)\n",
    "    \n",
    "    #find sections\n",
    "    sections = scrapper.find_all(\"section\",attrs={\"id\":section_matcher})\n",
    "    \n",
    "    #print(f\"sections = {len(sections)}\")\n",
    "    url_data = []\n",
    "    for ind,section in enumerate(sections):\n",
    "        #category of article i.e. sports, pollution\n",
    "        category = section.find(\"a\",attrs={\"class\":\"section-list-heading\"}).text.strip()\n",
    "        #print(f\"{ind+1} - {category}\")\n",
    "        #article urls\n",
    "        for url in section.find(\"ul\",class_ = \"archive-list\").find_all(\"a\"):\n",
    "            url_data.append({\"url\":url.get(\"href\"),\"title\":url.text,\"date\":datetime_obj,\"category\":category,\"media\":\"hindu\"})\n",
    "        \n",
    "    return url_data\n",
    "    \n",
    "#scrape author & city from article\n",
    "def hindu_get_author_and_city(html_resp):\n",
    "    soap = bs4.BeautifulSoup(html_resp,\"html.parser\")\n",
    "    author_section = soap.find(\"div\",class_=\"author-container hidden-xs\")\n",
    "    author = author_section.find(\"span\",class_=\"author-img-name 1\")\n",
    "    if author:\n",
    "        author = author.text.strip()\n",
    "        #remove white-space & comma\n",
    "        author = re.sub(r\"[,*\\s*\\n*]\",\"\",author)\n",
    "    else:\n",
    "        author = None\n",
    "    place_time = author_section.find(\"div\",class_ = \"ut-container\").find_all(\"span\")\n",
    "    #print(author,place_time[0].text.strip())\n",
    "    city = None\n",
    "    if len(place_time) >= 3:\n",
    "        city = place_time[0].text.strip()\n",
    "        #remove white-space & comma\n",
    "        city = re.sub(r\"[,*\\s*\\n*:*]\",\"\",city)\n",
    "    return [author,city]      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions between TOI & Hindu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex matching string:\n",
      " air\\spollution | air\\squality | aqi | carbon\\smonoxide | pm2.5 | pm10 | ozone | pm\\s2.5 | pm\\s10 | stubble\\sburning | smog | sulphur\\sdioxide | nitrogen\\sdioxide | so2 | air\\spollutants | acid\\srain | crop\\sburning | pm2·5 | odd\\seven | airpollution | car\\semissions | oddeven | airquality\n"
     ]
    }
   ],
   "source": [
    "def get_regex_string_from_single_word(keyword,space=False,dostrip=True):\n",
    "    \n",
    "    #to check empty string before the keyword\n",
    "    re_str = \"\"\n",
    "    space = \"\"\n",
    "    if space:\n",
    "        space = \"\\\\b\"\n",
    "        re_str+=space\n",
    "    keyword = keyword.lower()\n",
    "    if dostrip:\n",
    "        keyword = keyword.strip()\n",
    "    words = keyword.split(\" \")\n",
    "    n = len(words)\n",
    "    for ind,word in enumerate(words):\n",
    "        re_str+=word\n",
    "        if ind+1 < n:\n",
    "            #to accept white space between two word\n",
    "            re_str+=\"\\\\s\"\n",
    "    \n",
    "    #to check empty string after the keyword\n",
    "    re_str+= space\n",
    "    return re_str\n",
    "\n",
    "\n",
    "keyword_df = pd.read_csv(\"airpollution_keywords.csv\")\n",
    "keyword_df[\"regex_string\"] = keyword_df[\"keywords\"].apply(get_regex_string_from_single_word)\n",
    "keyword_df[\"keywords\"] = keyword_df[\"keywords\"].apply(lambda x:x.lower())\n",
    "keywords_count = dict(zip(keyword_df.keywords, [0]*len(keyword_df.keywords)))\n",
    "#get regex matching string\n",
    "keywords_match_string = \"\"\n",
    "for each in keyword_df[\"regex_string\"]:\n",
    "    keywords_match_string+=each\n",
    "    keywords_match_string+=\" | \"\n",
    "keywords_match_string = keywords_match_string[:-3]\n",
    "print(\"regex matching string:\\n\",keywords_match_string)\n",
    "\n",
    "\n",
    "def get_air_quality_keywords(text,match_string):\n",
    "    matcher = re.compile(match_string,flags=re.I | re.X)\n",
    "    matches = matcher.findall(text)\n",
    "    \n",
    "    #update count in dict\n",
    "    count = {}\n",
    "    for each in matches:\n",
    "        key = each.strip().lower()\n",
    "        try:\n",
    "            count[key]+=1\n",
    "        except:\n",
    "            count[key] = 1\n",
    "    return count\n",
    "\n",
    "def scrape_airquality_article(url_data):\n",
    "    if \"//ads.\" in url_data[\"url\"]:\n",
    "        return [\"aid\",\"no macthes\"]\n",
    "    author = \"\"\n",
    "    category = \"\"\n",
    "    city = \"\"\n",
    "    try:\n",
    "        url,title,datetime_obj = url_data[\"url\"].strip(), url_data[\"title\"], url_data[\"date\"]\n",
    "        if \"category\" in url_data:\n",
    "            category = url_data[\"category\"]\n",
    "        #newspaper3k library\n",
    "        article = Article(url,config = config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        #article text not found\n",
    "        if len(article.text.strip()) < 10:\n",
    "            return [\"notfound\",url_data]\n",
    "        #airuality related keywords\n",
    "        matches = get_air_quality_keywords(article.text,keywords_match_string)\n",
    "\n",
    "        articles_json = {}\n",
    "        #atleast one keyword found\n",
    "        if len(matches) > 0:\n",
    "            print(f\"Found in {category}\\n{title}\")\n",
    "            #print(url)\n",
    "            \n",
    "            #get author & city\n",
    "            if url_data[\"media\"] == \"hindu\":\n",
    "                author, city = hindu_get_author_and_city(article.html)\n",
    "            if url_data[\"media\"] == \"TOI\":\n",
    "                author, city = TOI_get_author_and_city(article.html)\n",
    "            #print(f\"Author = {author}\\nCity = {city}\")       \n",
    "            articles_json = {\"date\":datetime_obj.date(),\n",
    "                        \"url\":url,\n",
    "                        \"heading\":title,\n",
    "                        \"content\":article.text,\n",
    "                        \"other\":{\"author\":author,\"city\":city,\"top_image\":article.top_image,\n",
    "                                 \"category\":category,\"keywords\":article.meta_keywords,\n",
    "                                 \"tags\":list(article.tags)},\n",
    "                        \"matches\":matches\n",
    "                        }\n",
    "            return [articles_json,article.html]\n",
    "        return [articles_json,\"No macthes\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "       \n",
    "        url_data[\"exception_title\"] = str(type(e))\n",
    "        url_data[\"exception_info\"] = traceback.format_exc()\n",
    "        print(\"****************************************Exception***********************************************\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"***************************************************************************************\")\n",
    "        return [\"exception\",url_data]\n",
    "    \n",
    "def store_article_data(articles,fname):\n",
    "    #load content_not_found_urls [list]\n",
    "    content_not_found_urls = []\n",
    "    \n",
    "    #load data of air_quality_articles [list]\n",
    "    air_quality_articles = []\n",
    "    \n",
    "    #html responses\n",
    "    html_resp = []\n",
    "\n",
    "    #load urls where exceptions were occured\n",
    "    exceptions = []\n",
    "\n",
    "    \n",
    "    for each in articles:\n",
    "        #if it is air quality related article\n",
    "        if type(each[0]) == dict and each[0] != {}:\n",
    "            #store structured json scrapped data\n",
    "            air_quality_articles.append(each[0])\n",
    "\n",
    "            #store html_text_response of page\n",
    "            html_json = {\"heading\":each[0][\"heading\"],\"html\":each[1]}\n",
    "            html_resp.append(html_json)\n",
    "\n",
    "        #exception occured in get request \n",
    "        elif \"exception\" in each[0]:\n",
    "            print(\"stored Exception\")\n",
    "            exceptions.append(each[1])\n",
    "\n",
    "        #content not found\n",
    "        elif \"notfound\" in each[0]:\n",
    "            content_not_found_urls.append(each[1])\n",
    "\n",
    "    \n",
    "    #dump html responses\n",
    "    pickle.dump(html_resp,open(HtmlResponses+fname,\"wb\"))\n",
    "\n",
    "    \n",
    "    #store all the data in respective pickle file\n",
    "    pickle.dump(air_quality_articles,open(Airquality+fname,\"wb\")) \n",
    "    pickle.dump(content_not_found_urls,open(ContentNotFound+fname,\"wb\")) \n",
    "    pickle.dump(exceptions,open(Exceptions+fname,\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date = 2021-01-01 00:00:00, total urls = 431\n",
      "-------------------------9.862453699111938 seconds - 2021-01-01 00:00:00----------------------------\n",
      "Date = 2021-01-02 00:00:00, total urls = 386\n",
      "-------------------------9.360830068588257 seconds - 2021-01-02 00:00:00----------------------------\n",
      "Date = 2021-01-03 00:00:00, total urls = 323\n",
      "-------------------------8.510723352432251 seconds - 2021-01-03 00:00:00----------------------------\n",
      "27.735363245010376\n"
     ]
    }
   ],
   "source": [
    "#number of cpus\n",
    "n_cpu = mp.cpu_count()\n",
    "\n",
    "#pool for multiprocessing\n",
    "pool = mp.Pool(processes=n_cpu)\n",
    "\n",
    "start_date = datetime(2021,1,1)\n",
    "end_date = datetime(2021,1,3)\n",
    "\n",
    "#distance between two dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while start_date <= end_date:\n",
    "    start1 = time.time()\n",
    "    \n",
    "    #additional info\n",
    "    fname = str(start_date.date())\n",
    "    try:\n",
    "        additional_info = pickle.load(open(ArticleData+\"additional_info\",\"rb\"))\n",
    "    except FileNotFoundError:\n",
    "        additional_info = {}\n",
    "        \n",
    "    additional_info[fname] = {}\n",
    "    additional_info[fname][\"start_timestamp\"] = str(datetime.today())\n",
    "    \n",
    "    #get articles' page response\n",
    "    status = hindu_get_article_list_html_resp(start_date)\n",
    "    if status[0] == \"exception\":\n",
    "        _,exception_title,exception_info = status\n",
    "        pickle.dump([start_date,exception_title,exception_title],open(HomeExceptions+fname,\"wb\"))\n",
    "    else:\n",
    "        resp, date = status\n",
    "        \n",
    "    \n",
    "    #extract only article's urls from article page\n",
    "    urls =  hindu_get_article_urls(resp,start_date)\n",
    "    total_urls = len(urls)\n",
    "    print(f\"Date = {str(start_date)}, total urls = {total_urls}\")\n",
    "        \n",
    "    #scrap content of each articles using multiprocessing\n",
    "    articles = pool.map(scrape_airquality_article,urls[:10]) \n",
    "    \n",
    "    #additional info\n",
    "    additional_info[fname][\"total\"] = total_urls\n",
    "    additional_info[fname][\"end_timestamp\"] = str(datetime.today())\n",
    "    pickle.dump(additional_info,open(ArticleData+\"additional_info\",\"wb\"))\n",
    "        \n",
    "    #store necessary data\n",
    "    store_article_data(articles,fname)\n",
    "    \n",
    "    \n",
    "    end1 = time.time()\n",
    "    print(f\"-------------------------{end1 - start1} seconds - {str(start_date)}----------------------------\")\n",
    "    start_date += delta\n",
    "    #time.sleep(10)\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping for  Exceptions\n",
    "* beacause some times url is valid but due to networking issue, content can not be scrappped so by doing again scrapping for that urls can scrap content for most of urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-31 00:07:48,446 - 0 urls in year-2011\n",
      "2021-05-31 00:07:48,477 - 0 urls in year-2012\n",
      "2021-05-31 00:07:48,502 - 0 urls in year-2013\n",
      "2021-05-31 00:07:48,528 - 2 urls in year-2014\n",
      "2021-05-31 00:07:48,990 - 0 urls in year-2015\n",
      "2021-05-31 00:07:49,018 - 0 urls in year-2016\n",
      "2021-05-31 00:07:49,053 - 0 urls in year-2017\n",
      "2021-05-31 00:07:49,080 - 66 urls in year-2018\n",
      "2021-05-31 00:07:49,908 - Found in delhi\n",
      "Delhi’s air quality continues to be in ‘poor’ category \n",
      "2021-05-31 00:07:51,359 - 0 urls in year-2019\n"
     ]
    }
   ],
   "source": [
    "#number of cpus\n",
    "n_cpu = mp.cpu_count()\n",
    "\n",
    "#pool for multiprocessing\n",
    "pool = mp.Pool(processes=n_cpu)\n",
    "for year in range(2011,2020):\n",
    "    start = datetime(year,1,1).date()\n",
    "    end = datetime(year,12,31).date()\n",
    "    delta = timedelta(days=1)\n",
    "    json_resp = []\n",
    "    while start<=end:\n",
    "        try:\n",
    "            json = pd.read_pickle(Exceptions+str(start))\n",
    "        except FileNotFoundError:\n",
    "            start+=delta\n",
    "            continue\n",
    "            \n",
    "        json_resp.extend(json)\n",
    "        start+=delta\n",
    "    print(f\"{len(json_resp)} urls in year-{year}\")\n",
    "    articles = pool.map(scrape_airquality_article,json_resp)\n",
    "    store_article_data(articles,f\"re_{str(start)}_{str(end)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10226.08913898468/60/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
