{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq newspaper3k\n",
    "import requests\n",
    "import bs4\n",
    "import urllib3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize\n",
    "import multiprocessing as mp\n",
    "import traceback\n",
    "import pickle\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "config = Config()\n",
    "config.request_timeout = 60\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "\n",
    "#import helper as TOI\n",
    "\n",
    "#starting date of TOI articles\n",
    "benchmark_date = datetime(2001,1,1)\n",
    "\n",
    "#day_id which is used to get url of any article\n",
    "#this id can be calculated using banchmark_date & benchmark_id\n",
    "benchmark_day_id = 36892\n",
    "\n",
    "#path to store articles\n",
    "ArticleData = \"ArticlesData/TOI/\"\n",
    "Path(ArticleData).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Airquality = ArticleData+\"Airquality/\"\n",
    "Path(Airquality).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ContentNotFound = ArticleData+\"ContentNotFound/\"\n",
    "Path(ContentNotFound).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Exceptions = ArticleData+\"Exceptions/\"\n",
    "Path(Exceptions).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HtmlResponses = ArticleData + \"HtmlResponses/\"\n",
    "Path(HtmlResponses).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "LOGGING_ENABLED = False\n",
    "if LOGGING_ENABLED:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    file_handler = logging.FileHandler('TOI_2.log', 'a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    print = logger.info\n",
    "\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scrap full text response of html page for given date\n",
    "#input: datetime object\n",
    "#output: List -> [date_string,html_page_response]\n",
    "\n",
    "def get_calender_day_response(datetime_obj):\n",
    "    date = datetime_obj\n",
    "    day = date.day\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    \n",
    "    #difference between current date & starting date(benchmark date)\n",
    "    diff = date - benchmark_date\n",
    "    \n",
    "    #day_id = banchmark_day_if + difference(calculated above)\n",
    "    day_id = benchmark_day_id + diff.days\n",
    "    \n",
    "    #generate a link using year, month, day, day_id\n",
    "    link =  f\"https://timesofindia.indiatimes.com/{year}/{month}/{day}/archivelist/year-{year},month-{month},starttime-{day_id}.cms\"\n",
    "    \n",
    "    #gettig html response\n",
    "    resp = requests.get(link).text\n",
    "    return [date,resp]\n",
    "\n",
    "#to extract only articles links (501 urls) from page_response which is scrapped by get_calender_day_response() function\n",
    "#input: [date,html_page_response]\n",
    "#output: List of shape :n_articles * 3 \n",
    "#        columns = [article_url,article_heading,article_date]\n",
    "\n",
    "def get_articles_urls(day_page_resp):\n",
    "    date = day_page_resp[0]\n",
    "    page_response = day_page_resp[1]\n",
    "    \n",
    "    #scrapper from BeautifulSoap\n",
    "    scrapper = bs4.BeautifulSoup(page_response,\"html.parser\")\n",
    "    \n",
    "    #extract all tables\n",
    "    tables = scrapper.find_all('table')\n",
    "    \n",
    "    #scrap 3rd table which contains 501 article urls\n",
    "    article_table = str(tables[2])\n",
    "    \n",
    "    #scrapper to scrap only article links from table\n",
    "    soup =  bs4.BeautifulSoup(article_table,\"html.parser\")\n",
    "    urls = soup.find_all(\"a\")\n",
    "    home_url = \"https://timesofindia.indiatimes.com\"\n",
    "    complete_urls = []\n",
    "    for each in urls:\n",
    "        url = each.get('href').strip()\n",
    "        #if url not starts with 'http' then append it with home_url\n",
    "        if url[:4] != \"http\":\n",
    "            url  = home_url + url\n",
    "            \n",
    "        #each.text gives heading of article\n",
    "        complete_urls.append({\"url\":url,\"title\":each.text,\"date\":date,\"media\":\"TOI\"})\n",
    "        \n",
    "    return complete_urls\n",
    "\n",
    "\n",
    "def TOI_get_author_and_city(resp):\n",
    "    scrapper =  bs4.BeautifulSoup(resp,\"html.parser\")\n",
    "    #find tag where category is mentioned\n",
    "    category_tag = scrapper.find_all(\"div\",class_ = [\"YhAlT\",\"navbdcrumb\"])\n",
    "    if category_tag == []:\n",
    "        category_tag = scrapper.find_all(\"div\",attrs = {\"id\":\"breadcrumb\"})\n",
    "\n",
    "    #find tag where author name is mentioned\n",
    "    author_tag = scrapper.find_all(\"div\",class_ = [\"yYIu- byline\",\"as_byline\",\"byline-content\"])\n",
    "\n",
    "    #make a string\n",
    "    author_str = \"\"\n",
    "    category_str = \"\"\n",
    "\n",
    "    for each in author_tag:\n",
    "        author_str += each.text\n",
    "\n",
    "    for each in category_tag:\n",
    "        category_str += each.text\n",
    "    #print(category_str,\"\\n\",author_str)\n",
    "    \n",
    "    return [author_str,category_str]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions between Hindu & TOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex matching string:\n",
      " air\\spollution | air\\squality | aqi | carbon\\smonoxide | pm2.5 | pm10 | ozone | pm\\s2.5 | pm\\s10 | stubble\\sburning | smog | sulphur\\sdioxide | nitrogen\\sdioxide | so2 | air\\spollutants | acid\\srain | crop\\sburning | pm2Â·5 | odd\\seven | airpollution | car\\semissions | oddeven | airquality\n"
     ]
    }
   ],
   "source": [
    "def get_regex_string_from_single_word(keyword,space=False,dostrip=True):\n",
    "    \n",
    "    #to check empty string before the keyword\n",
    "    re_str = \"\"\n",
    "    space = \"\"\n",
    "    if space:\n",
    "        space = \"\\\\b\"\n",
    "        re_str+=space\n",
    "    keyword = keyword.lower()\n",
    "    if dostrip:\n",
    "        keyword = keyword.strip()\n",
    "    words = keyword.split(\" \")\n",
    "    n = len(words)\n",
    "    for ind,word in enumerate(words):\n",
    "        re_str+=word\n",
    "        if ind+1 < n:\n",
    "            #to accept white space between two word\n",
    "            re_str+=\"\\\\s\"\n",
    "    \n",
    "    #to check empty string after the keyword\n",
    "    re_str+= space\n",
    "    return re_str\n",
    "\n",
    "\n",
    "keyword_df = pd.read_csv(\"airpollution_keywords.csv\")\n",
    "keyword_df[\"regex_string\"] = keyword_df[\"keywords\"].apply(get_regex_string_from_single_word)\n",
    "keyword_df[\"keywords\"] = keyword_df[\"keywords\"].apply(lambda x:x.lower())\n",
    "keywords_count = dict(zip(keyword_df.keywords, [0]*len(keyword_df.keywords)))\n",
    "#get regex matching string\n",
    "keywords_match_string = \"\"\n",
    "for each in keyword_df[\"regex_string\"]:\n",
    "    keywords_match_string+=each\n",
    "    keywords_match_string+=\" | \"\n",
    "keywords_match_string = keywords_match_string[:-3]\n",
    "print(\"regex matching string:\\n\",keywords_match_string)\n",
    "\n",
    "\n",
    "def get_air_quality_keywords(text,match_string):\n",
    "    matcher = re.compile(match_string,flags=re.I | re.X)\n",
    "    matches = matcher.findall(text)\n",
    "    \n",
    "    #update count in dict\n",
    "    count = {}\n",
    "    for each in matches:\n",
    "        key = each.strip().lower()\n",
    "        try:\n",
    "            count[key]+=1\n",
    "        except:\n",
    "            count[key] = 1\n",
    "    return count\n",
    "\n",
    "def scrape_airquality_article(url_data):\n",
    "    if \"//ads.\" in url_data[\"url\"]:\n",
    "        return [\"aid\",\"no macthes\"]\n",
    "    author = \"\"\n",
    "    category = \"\"\n",
    "    city = \"\"\n",
    "    try:\n",
    "        url,title,datetime_obj = url_data[\"url\"].strip(), url_data[\"title\"], url_data[\"date\"]\n",
    "        if \"category\" in url_data:\n",
    "            category = url_data[\"category\"]\n",
    "        #newspaper3k library\n",
    "        article = Article(url,config = config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        #article text not found\n",
    "        if len(article.text.strip()) < 10:\n",
    "            return [\"notfound\",url_data]\n",
    "        #airuality related keywords\n",
    "        matches = get_air_quality_keywords(article.text,keywords_match_string)\n",
    "\n",
    "        articles_json = {}\n",
    "        #atleast one keyword found\n",
    "        if len(matches) > 0:\n",
    "            print(f\"Found in {category}\\n{title}\")\n",
    "            #print(url)\n",
    "            \n",
    "            #get author & city\n",
    "            if url_data[\"media\"] == \"hindu\":\n",
    "                author, city = hindu_get_author_and_city(article.html)\n",
    "            if url_data[\"media\"] == \"TOI\":\n",
    "                author, city = TOI_get_author_and_city(article.html)\n",
    "            #print(f\"Author = {author}\\nCity = {city}\")       \n",
    "            articles_json = {\"date\":datetime_obj.date(),\n",
    "                        \"url\":url,\n",
    "                        \"heading\":title,\n",
    "                        \"content\":article.text,\n",
    "                        \"other\":{\"author\":author,\"city\":city,\"top_image\":article.top_image,\n",
    "                                 \"category\":category,\"keywords\":article.meta_keywords,\n",
    "                                 \"tags\":list(article.tags)},\n",
    "                        \"matches\":matches\n",
    "                        }\n",
    "            return [articles_json,article.html]\n",
    "        #print(\"found\")\n",
    "        return [articles_json,\"No macthes\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "    \n",
    "        url_data[\"exception_title\"] = str(type(e))\n",
    "        url_data[\"exception_info\"] = traceback.format_exc()\n",
    "        print(\"****************************************Exception***********************************************\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"***************************************************************************************\")\n",
    "        return [\"exception\",url_data]\n",
    "    \n",
    "def store_article_data(articles,fname):\n",
    "    #load content_not_found_urls [list]\n",
    "    content_not_found_urls = []\n",
    "    \n",
    "    #load data of air_quality_articles [list]\n",
    "    air_quality_articles = []\n",
    "    \n",
    "    #html responses\n",
    "    html_resp = []\n",
    "\n",
    "    #load urls where exceptions were occured\n",
    "    exceptions = []\n",
    "\n",
    "    \n",
    "    for each in articles:\n",
    "        #if it is air quality related article\n",
    "        if type(each[0]) == dict and each[0] != {}:\n",
    "            #store structured json scrapped data\n",
    "            air_quality_articles.append(each[0])\n",
    "\n",
    "            #store html_text_response of page\n",
    "            html_json = {\"heading\":each[0][\"heading\"],\"html\":each[1]}\n",
    "            html_resp.append(html_json)\n",
    "\n",
    "        #exception occured in get request \n",
    "        elif \"exception\" in each[0]:\n",
    "            exceptions.append(each[1])\n",
    "\n",
    "        #content not found\n",
    "        elif \"notfound\" in each[0]:\n",
    "            content_not_found_urls.append(each[1])\n",
    "\n",
    "    \n",
    "    #dump html responses\n",
    "    pickle.dump(html_resp,open(HtmlResponses+fname,\"wb\"))\n",
    "\n",
    "    \n",
    "    #store all the data in respective pickle file\n",
    "    pickle.dump(air_quality_articles,open(Airquality+fname,\"wb\")) \n",
    "    pickle.dump(content_not_found_urls,open(ContentNotFound+fname,\"wb\")) \n",
    "    pickle.dump(exceptions,open(Exceptions+fname,\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping of Articles from start date to end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date = 2021-01-01 00:00:00, total urls = 501\n",
      "-------------------------2.339319944381714 seconds - 2021-01-01 00:00:00----------------------------\n",
      "Date = 2021-01-02 00:00:00, total urls = 501\n",
      "-------------------------2.237348794937134 seconds - 2021-01-02 00:00:00----------------------------\n",
      "Date = 2021-01-03 00:00:00, total urls = 501\n",
      "-------------------------2.0526089668273926 seconds - 2021-01-03 00:00:00----------------------------\n",
      "1626143035.603887\n"
     ]
    }
   ],
   "source": [
    "#number of cpus\n",
    "n_cpu = mp.cpu_count()\n",
    "\n",
    "#pool for multiprocessing\n",
    "pool = mp.Pool(processes=n_cpu)\n",
    "\n",
    "#enter start date and end date\n",
    "start_date = datetime(2021,1,1)\n",
    "end_date = datetime(2021,1,3)\n",
    "home_page = []\n",
    "\n",
    "#distance between two dates\n",
    "delta = timedelta(days=1)\n",
    "start = time.time()\n",
    "\n",
    "while start_date <= end_date:\n",
    "    start1 = time.time()\n",
    "    \n",
    "    #get articles' page response\n",
    "    try:\n",
    "        resp = get_calender_day_response(start_date)  \n",
    "    except Exception as e:\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Home Page Exception @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(traceback.format_exc())\n",
    "        home_page.append([start_date,str(type(e)),traceback.format_exc()])\n",
    "    \n",
    "    #extract only article's urls from article page\n",
    "    urls =  get_articles_urls(resp)\n",
    "    total_urls = len(urls)\n",
    "    print(f\"Date = {str(start_date)}, total urls = {total_urls}\")\n",
    "    \n",
    "    \n",
    "    fname = str(start_date.date())\n",
    "    try:\n",
    "        additional_info = pickle.load(open(ArticleData+\"additional_info\",\"rb\"))\n",
    "    except FileNotFoundError:\n",
    "        additional_info = {}\n",
    "        pickle.dump(additional_info,open(ArticleData+\"additional_info\",\"wb\"))\n",
    "    \n",
    "    additional_info[fname] = {}\n",
    "    additional_info[fname][\"start_timestamp\"] = str(datetime.today())  \n",
    "    additional_info[fname][\"total\"] = total_urls\n",
    "    \n",
    "    #scrap content of each articles using multiprocessing\n",
    "    articles = pool.map(scrape_airquality_article,urls[:10])   \n",
    "    \n",
    "    #additional info\n",
    "    additional_info[fname][\"end_timestamp\"] = str(datetime.today())\n",
    "    pickle.dump(additional_info,open(ArticleData+\"additional_info\",\"wb\"))\n",
    "    \n",
    "    store_article_data(articles,fname)\n",
    "\n",
    "    \n",
    "    end1 = time.time()\n",
    "    print(f\"-------------------------{end1 - start1} seconds - {str(start_date)}----------------------------\")\n",
    "    start_date += delta\n",
    "    #time.sleep(10)\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "print((end - start)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(home_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping for  Exceptions\n",
    "* beacause some times url is valid but due to networking issue, content can not be scrappped so by doing again scrapping for that urls can scrap content for most of urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of cpus\n",
    "n_cpu = mp.cpu_count()\n",
    "\n",
    "#pool for multiprocessing\n",
    "pool = mp.Pool(processes=n_cpu)\n",
    "for year in range(2021,2022):\n",
    "    start = datetime(year,1,1).date()\n",
    "    end = datetime(year,12,31).date()\n",
    "    delta = timedelta(days=1)\n",
    "    json_resp = []\n",
    "    while start<=end:\n",
    "        try:\n",
    "            json = pd.read_pickle(Exceptions+str(start))\n",
    "        except FileNotFoundError:\n",
    "            start+=delta\n",
    "            continue\n",
    "        start+=delta  \n",
    "        json_resp.extend(json)\n",
    "        \n",
    "    print(f\"{len(json_resp)} urls in year-{year}\")\n",
    "    #re scrape all articles\n",
    "    articles = pool.map(scrape_airquality_article,json_resp)\n",
    "    \n",
    "    #store articles if found\n",
    "    store_article_data(articles,f\"re_{str(start)}_{str(end-delta)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
